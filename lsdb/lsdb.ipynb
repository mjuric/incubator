{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4da47d",
   "metadata": {},
   "source": [
    "## Prototyping Dask-HiPSCat Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5a75925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the best way to learn/hack on Dask seems to be by directly editing the installed source code,\n",
    "# let's have it all reloaded automatically.\n",
    "#\n",
    "# We'll exempt modules where we monkeypatch functions from autoreloading (e.g. dask.utils).\n",
    "# Otherwise the autoreload would remove our patched version.\n",
    "#\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%aimport -dask.utils\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac3f1aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.__version__='2022.12.1'\n",
      "pd.__version__='1.5.2'\n",
      "np.__version__='1.24.0'\n",
      "hp.__version__='1.16.1'\n"
     ]
    }
   ],
   "source": [
    "# To set up an environment for this experiment, run:\n",
    "#    mamba create -n lsd2-2023 -c conda-forge dask pyarrow healpy ipykernel python-graphviz rich ipywidgets\n",
    "# and then create a Jupyter kernel w. something like:\n",
    "#    conda activate lsd2-2023\n",
    "#    python -m ipykernel install --user --name lsd2-2023 --display-name \"LSD2 (2023)\"\n",
    "#\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "\n",
    "print(f\"{dask.__version__=}\")\n",
    "print(f\"{pd.__version__=}\")\n",
    "print(f\"{np.__version__=}\")\n",
    "print(f\"{hp.__version__=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67828a8b",
   "metadata": {},
   "source": [
    "It's useful to have a unique (within the catalog!) ID for each row. Two reasons: tables can be efficiently joined on that ID (e.g., a table of objects with its corresponding sources), and it can also be used to determine the partition where that row lives (maybe w. some extra metadata).\n",
    "\n",
    "For now, I'll just make that be a very high-order healpix ipix index computed from the object's (ra, dec) and maximally bitshifted to the left, with the remaining bits to the right just increasing numerically if there are more than one objects with the same ipix in the catalog.\n",
    "\n",
    "This is implemented by `compute_index` (and should be performant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de1938a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order=20\n",
      "pix_edge_len_arcsec=0.2012980319424261\n",
      "bits=44\n",
      "maxindex=1048576\n"
     ]
    }
   ],
   "source": [
    "# some stats on high-order pixels\n",
    "# for order=20, ipix corresponds to 0.2\" x 0.2\" area in the sky (~a single LSST pixel)\n",
    "# and we have room for up to ~1M objects detected within that same pixel.\n",
    "#\n",
    "# FIXME: while I can't yet imagine a scenario with 1M objects in 0.2\" x 0.2\" area, any such limitation feels icky.\n",
    "#        someone at some point will come up with a need for more... we should think of an escape hatch...\n",
    "#\n",
    "order=20\n",
    "pix_edge_len_arcsec = np.sqrt(hp.nside2pixarea(hp.order2nside(order), degrees=True))*3600 # arsec x arcsec square\n",
    "bits=4 + 2*order\n",
    "maxindex=2**(64-bits)\n",
    "print(f\"{order=}\\n{pix_edge_len_arcsec=}\\n{bits=}\\n{maxindex=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccf9a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_index(ra, dec, order=20):\n",
    "    # the 64-bit index, viewed as a bit array, consists of two parts:\n",
    "    #\n",
    "    #    idx = |(pix)|(rank)|\n",
    "    #\n",
    "    # where pix is the healpix nest-scheme index of for given order,\n",
    "    # and rank is a monotonically increasing integer for all objects\n",
    "    # with the same value of pix.\n",
    "\n",
    "    # compute the healpix pix-index of each object\n",
    "    pix = hp.ang2pix(2**order, ra, dec, nest=True, lonlat=True)\n",
    "\n",
    "    # shift to higher bits of idx\n",
    "    bits=4 + 2*order\n",
    "    idx = pix.astype(np.uint64) << (64-bits)\n",
    "\n",
    "    # sort\n",
    "    orig_idx = np.arange(len(idx))\n",
    "    sorted_idx = np.lexsort((dec, ra, idx))\n",
    "    idx, ra, dec, orig_idx = idx[sorted_idx], ra[sorted_idx], dec[sorted_idx], orig_idx[sorted_idx]\n",
    "\n",
    "    # compute the rank for each unique value of idx (== bitshifted pix, at this point)\n",
    "    # the goal: given values of idx such as:\n",
    "    #   1000, 1000, 1000, 2000, 2000, 3000, 5000, 5000, 5000, 5000, ...\n",
    "    # compute a unique array such as:\n",
    "    #   1000, 1001, 1002, 2000, 2001, 3000, 5000, 5001, 5002, 5003, ...\n",
    "    # that is for the subset of nobj objects with the same pix, add\n",
    "    # to the index an range [0..nobj)\n",
    "    #\n",
    "    # how this works:\n",
    "    # * x are the indices of the first appearance of a new pix value. In the example above,\n",
    "    # it would be equal to [0, 3, 5, 6, ...]. But note that this is also the total number\n",
    "    # of entries before the next unique value (e.g. 5 above means there were 5 elements in\n",
    "    # idx -- 1000, 1000, 1000, 2000, 2000 -- before the third unique value of idx -- 3000)\n",
    "    # * i are the indices of each unique value of idx, starting with 0 for the first one\n",
    "    # in the example above, i = [0, 0, 0, 1, 1, 2, 3, 3, 3, 3]\n",
    "    # * we need construct an array such as [0, 1, 2, 0, 1, 0, 0, 1, 2, 3, ...], i.e.\n",
    "    # a one that resets every time the value of idx changes. If we can construct this, we\n",
    "    # can add this array to idx and achieve our objective.\n",
    "    # * the way to do it: start with a monotonously increasing array\n",
    "    #  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...] and subtract an array that looks like this:\n",
    "    #  [0, 0, 0, 3, 3, 5, 6, 7, 7, 7, ...]. This is an array that at each location has\n",
    "    #  the index where that location's pix value appeared for the first time. It's easy\n",
    "    #  to confirm that this is simply x[i].\n",
    "    #\n",
    "    # And this is what the following four lines implement.\n",
    "    _, x, i = np.unique(idx, return_inverse=True, return_index=True)\n",
    "    x = x.astype(np.uint64)\n",
    "    ii = np.arange(len(i), dtype=np.uint64)\n",
    "    di = ii - x[i]\n",
    "    idx += di\n",
    "\n",
    "    # remap back to the old sort order\n",
    "    idx = idx[orig_idx]\n",
    "\n",
    "    return idx\n",
    "\n",
    "if False:\n",
    "    # quick test\n",
    "    try:\n",
    "        df_orig\n",
    "    except NameError:\n",
    "        #df_orig = pd.read_parquet('/epyc/projects3/sam_hipscat/output/gaia_real/Norder6/Npix29079/catalog.parquet')\n",
    "        df_orig = pd.read_parquet('/epyc/projects3/sam_hipscat/output/gaia_real/Norder2/Npix138/catalog.parquet')\n",
    "    dff = df_orig.copy()\n",
    "    dff[\"_ID\"] = compute_index(dff[\"ra\"].values, dff[\"dec\"].values, order=14)\n",
    "    dff.set_index(\"_ID\", inplace=True)\n",
    "    dff.sort_index(inplace=True)\n",
    "\n",
    "    from IPython.display import display\n",
    "    display(dff.iloc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e39aef",
   "metadata": {},
   "source": [
    "Sam's current conversion of Gaia doesn't include an index column. We'll have to clone his catalog and add it here. That's what the next few cells do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "384b730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test parquet writing with metadata\n",
    "def add_index(infile, outfile):\n",
    "    df = pd.read_parquet(infile)\n",
    "\n",
    "    df[\"_ID\"] = compute_index(df[\"ra\"].values, df[\"dec\"].values, order=14)\n",
    "    df.set_index(\"_ID\", inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    import os, os.path\n",
    "    os.makedirs(os.path.dirname(outfile), exist_ok=True)\n",
    "\n",
    "    df.to_parquet(outfile, engine='pyarrow', compression='snappy', index=True)\n",
    "\n",
    "def fixup_hipscat(inprefix, outprefix, start=None, end=None):\n",
    "    # the [start:end] at the end lets you convert a subset of\n",
    "    # partitions, to speed things up while developing\n",
    "\n",
    "    incat = glob.glob(f'{inprefix}/*/*/catalog.parquet')\n",
    "    outcat = [ outprefix + fn[len(inprefix):] for fn in incat ]\n",
    "\n",
    "    from rich.progress import track\n",
    "    for infile, outfile in track(list(zip(incat, outcat))[start:end]):\n",
    "        add_index(infile, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643c00c9",
   "metadata": {},
   "source": [
    "This creates two catalogs, so we can test `join`s later on.\n",
    "\n",
    "(note: rich.progress seems to render double progress bars for me; I found https://github.com/Textualize/rich/issues/1737 which claims this has been fixed, but doesn't seem so in this situation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0145a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "inprefix = \"/epyc/projects3/sam_hipscat/output/gaia_real\"\n",
    "outprefix1 = \"/epyc/projects3/mjuric_hipscat/gaiaA\"\n",
    "outprefix2 = \"/epyc/projects3/mjuric_hipscat/gaiaB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b4891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixup_hipscat(inprefix, outprefix1, 0, 100)\n",
    "fixup_hipscat(inprefix, outprefix2, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55679810",
   "metadata": {},
   "source": [
    "For our next trick, we'll use Dask's built-in `dd.read_parquet` to read our HiPSCat as a single parquet dataset.\n",
    "\n",
    "We will also have dask discover the min/max values the index (`_ID`) column in each .parquet file, by reading parquet metadata (as opposed to loading the entire file). This will allow dask to compute its `DataFrame.divisions` field, which tells it which files contain what range of indices. This, in turn, allows for rapid selection based on the index, as Dask can just load the files having the selected data.\n",
    "\n",
    "For this to work, a few things need to be true:\n",
    "1. There has to be an index column in each file (✔)\n",
    "1. Ranges of indices contained in individual partition must not overlap (✔, by construction of HiPSCat & our index)\n",
    "1. The parquet files must be read in increasing order of index (see below)\n",
    "\n",
    "If any of these (poorly documented) requirements are violated, Dask will still load the dataset but won't load the `DataFrame.divisions` field making it horribly slow. It also won't give any indication as to why it failed to load divisions -- i.e., it took me an hour to find a bug where one file had out-of-order indices. This is an example of unfortunate UX design. IMHO, if something is explicitly requested, and it fails, the code should complain loudly to let the user know something's off (raise an exception pointing to the problem). Here, Dask doesn't even emit a warning. This type of thing (silent failures w/o information on errors) are a common antipattern in Dask; caveat emptor.\n",
    "\n",
    "Now for our point 3. above: when passed a list of files (or a directory), Dask will sort them using natural sort (https://en.wikipedia.org/wiki/Natural_sort_order) hoping this file order is also the order in which indices increase in each file. This works for the typical case of `(part-1.parquet, part-2.parquet, ... part-9.parquet, part-10.parquet, ...)`. Directory names are also included in this sort. But this is where there's an issue with HiPSCat as (for example) `Norder1/Npix11/catalog.parquet` will contain objects with indices that are larger than (say) `Norder2/Npix0/catalog.parquet` (Healpix indices start at zero at the north pole and roughly increase towards the south pole).\n",
    "\n",
    "So we need to teach Dask to detect HiPSCat catalogs and sort their filenames differently. We'll do this by monkeypatching `dask.utils.natural_sort_key`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c69e989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch the dask.utils.natural_sort_key to recognize and sort hierarchical HiPS directories\n",
    "# in order of increasing healpix index of their contents.\n",
    "\n",
    "import dask.utils as du\n",
    "try:\n",
    "    _orig_natural_sort_key\n",
    "except NameError:\n",
    "    _orig_natural_sort_key = du.natural_sort_key\n",
    "\n",
    "def hips_or_natural_sort_key(s: str) -> list[str | int]:\n",
    "    import re\n",
    "    m = re.match(r\"^(.*)/Norder(\\d+)/Npix(\\d+)/([^/]*)$\", s)\n",
    "    if m is None:\n",
    "        return _orig_natural_sort_key(s)\n",
    "    \n",
    "    root, order, ipix, leaf = m.groups()\n",
    "    order, ipix = int(order), int(ipix)\n",
    "    ipix20 = ipix << 2*(20 - order)\n",
    "    k = (root, ipix20, leaf)\n",
    "    return k\n",
    "hips_or_natural_sort_key.__doc__ = _orig_natural_sort_key.__doc__\n",
    "\n",
    "du.natural_sort_key = hips_or_natural_sort_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438881a",
   "metadata": {},
   "source": [
    "Let's check that this works -- if it does, `df.divisions` will be a list of indices. Note `calculate_divisions=True` below; this requests this metadata to be loaded.\n",
    "\n",
    "(I also specify which `columns` to load, just to speed up development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b82acf0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndivisions=101: (8589934592, 72057602627862528, 126100802451275776, 144115505903435776, 216172782113783808) ...\n",
      "61472247\n"
     ]
    }
   ],
   "source": [
    "df = dd.read_parquet(f'{outprefix1}/*/*/catalog.parquet', calculate_divisions=True, columns=['ra', 'dec', 'source_id', 'parallax'])\n",
    "print(f\"ndivisions={len(df.divisions)}: {df.divisions[:5]} ...\")\n",
    "assert df.divisions and df.divisions[0] is not None\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "066a3d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndivisions=11: (8589934592, 2449958424922816512, 4683743706954596352, 4755801335352262656, 4827858873555615744) ...\n",
      "6961555\n"
     ]
    }
   ],
   "source": [
    "df2 = dd.read_parquet(f'{outprefix2}/*/*/catalog.parquet', columns=['ra', 'dec', 'source_id', 'parallax'], calculate_divisions=True)\n",
    "print(f\"ndivisions={len(df2.divisions)}: {df2.divisions[:5]} ...\")\n",
    "assert df2.divisions and df2.divisions[0] is not None\n",
    "print(len(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250f255f",
   "metadata": {},
   "source": [
    "And now let's demonstrate we can use Dask's own machinery to do something interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c79b9f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b1695d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 14.69 s\n"
     ]
    }
   ],
   "source": [
    "df.join(df2, rsuffix=\"_2\", how=\"inner\").query(\"ra_2 >= 45 and dec > 25\").\\\n",
    "    to_parquet(f'{outprefix2}-dask', overwrite=True, write_metadata_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f1a273",
   "metadata": {},
   "source": [
    "The output is just plain partitioned parquet, with one file per partition. Note that most of these ended up being empty; Dask doesn't cull them by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab91a5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 38732\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.61.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.57.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.1.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.2.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.58.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.4.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.5.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.6.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.0.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.7.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.3.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.8.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.84.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.10.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.12.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.9.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.11.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.13.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.42.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.44.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.41.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.43.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.39.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.45.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.40.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.49.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.48.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.50.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.46.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.52.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.53.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.47.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.51.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.60.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.63.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.76.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.62.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.77.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.79.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.64.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.81.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.78.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.59.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.67.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.83.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.87.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.68.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.69.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.71.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.70.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.73.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.65.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric  6676173 Dec 28 16:08 part.38.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric 32103129 Dec 28 16:08 part.75.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.20.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.56.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.37.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.74.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.14.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.98.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.36.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.66.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.80.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.21.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.19.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.35.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.28.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.89.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.24.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.34.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.90.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.16.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.33.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.32.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.55.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.96.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.91.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.27.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.93.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.29.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.31.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.26.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.15.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.94.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.25.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.18.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.99.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.86.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.97.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.22.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.17.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.82.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.72.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.88.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.23.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.92.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.85.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.100.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.30.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.95.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4962 Dec 28 16:08 part.54.parquet\r\n",
      "-rw-rw-r-- 1 mjuric mjuric     4024 Dec 28 16:08 _common_metadata\r\n",
      "-rw-rw-r-- 1 mjuric mjuric    61927 Dec 28 16:08 _metadata\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lrt '{outprefix2}-dask'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed35c1d",
   "metadata": {},
   "source": [
    "What have we learned?\n",
    "\n",
    "The good:\n",
    "\n",
    "* Dask can be taught to read HiPSCat parquet as if they're partitioned datasets\n",
    "* It involves some nasty monkeypatching of code, but not too much (one can imagine a PR to generalize the sort order)\n",
    "* Once loaded, things like simple joins work as expected\n",
    "\n",
    "The bad:\n",
    "* Dask's DataFrame partition model is rather inefficient (for our use case). It only records the _minmum_ index value of data each partition, but not the maximum. For example, given indices `df2.divisions = [8589934592, 2449958424922816512, 4683743706954596352, 4755801335352262656, 4827858873555615744, ...]`, it assumes that the first partition may contain any value between `[8589934592, 2449958424922816512)` -- essentially the entire northern sky. But it doesn't know that the maximum value in that partition is only around ~10000000000). So when it computes the execution graph for `join`, rather than immediately exluding many partitions from `df2`, it executes each and every one of those sub-joins (which all return empty dataframes). This can be seen by running `.visualize()` on the invocation above. This was discussed within the Dask community (https://github.com/dask/dask/issues/3384), but the conclusion was that it's a niche use case. Unfortunately, it's a very frequent, major, use case for us.\n",
    "* Dask doesn't appear to have a mature optimizer like Spark's Catalyst, that could perform predicate push-down to (for example) eliminate all unused columns from parquet reads. To _not_ read everything, one has to explicitly specify the columns one will use in `read_parquet`'s `columns` kwarg, and this API is optional -- left unspecified, it defaults to reading everything. Users are bound to forget this :(, and likely read 10x more data than they really need (costing both time and money). We need to fix this in our API (e.g., require colums to be specified in any future `read_hipscat` or alike).\n",
    "\n",
    "The ugly:\n",
    "* The documentation of internals is extremely poor. For example, the dask version used here uses a mysterious `BlockIndex` instance as an argument to `df.map_partitions` in it's `to_parquet` implementation. This instance behaves \"magically\" -- in the actual call it ends up being replaced by an index of a partition being mapped. This isn't documented anywhere -- official documentation says to use `partition_info` to achieve this. After reading the source code and inserting print statements one can figure out what BlockIndex does, but the lack of documentation is a huge time sink. It also points to Dask's development model not having a requirement that the corresponding documentation is updated with each PR (a lesson to us NOT do to that)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f61fa",
   "metadata": {},
   "source": [
    "Lessons learned and what do we do next?\n",
    "\n",
    "1. The main purpose of this exercise was to figure out how far could we get if we relied on Dask's `df.divisions` for efficient large-dataset computations. While it's nice to see going from zero to a simple join in ~24 hrs, the `df.divisions` data model (just minimums, no maximums) makes it unlikely we can build a performant class around it. We'll unfortunately have to override division handling in its entirety.\n",
    "\n",
    "  Fortunately, there's an example to follow! It looks like https://github.com/geopandas/dask-geopandas did something like this for geospatial dataframes. I haven't had much time to analyze their [code](https://github.com/geopandas/dask-geopandas/blob/main/dask_geopandas/core.py), but I see mentions of `spatial_partitions` which gives me hope).\n",
    "\n",
    "1. I now have a much better idea of what a good API may look like. I think we'll have something like `HiPSDataFrame` class, and want to do something like for analytics:\n",
    "  \n",
    "```\n",
    "df  = lsdb.read_hipscat(outprefix1, columns=['ra', 'dec', 'source_id', 'parallax'])\n",
    "df2 = lsdb.read_hipscat(outprefix2, columns=['ra', 'dec', 'source_id', 'parallax'])\n",
    "df.join(df2, rsuffix=\"_2\", how=\"inner\").query(\"ra_2 >= 45 and dec > 25\").to_hipscat(f'{outprefix2}-dask')\n",
    "```  \n",
    "  for analytics, and something like\n",
    "```\n",
    "df = dd.read_csv(\"gaia_inputs/*.csv\")\n",
    "hcat = lsdb.from_dask(df, lon=\"ra\", lat=\"dec\")\n",
    "hcat.to_hipscat('gaia')\n",
    "```\n",
    "for importing of catalogs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSD2 2023",
   "language": "python",
   "name": "python3-lsd2-2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
